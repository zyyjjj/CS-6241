{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS 6241 Homework 2\n",
    "### Implementing iterated rank-1 tensor decomposition to interpret time-series Beijing air quality data measured from different stations\n",
    "Yujia Zhang yz685@cornell.edu\n",
    "\n",
    "Data source: https://archive.ics.uci.edu/ml/datasets/Beijing+Multi-Site+Air-Quality+Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Implement rank-1 CP decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing rank-1 CP decomposition:\n",
    "# define a function (tensor, number of iterations)\n",
    "\n",
    "def CPdecomp(A, niter):\n",
    "    m,n,k = A.shape\n",
    "    x = np.random.rand(m)\n",
    "    y = np.random.rand(n)\n",
    "    z = np.random.rand(k)\n",
    "    \n",
    "    for iternum in range(niter):\n",
    "        # first minimize with respect to $x$\n",
    "        # then update x, and minimize with respect to y\n",
    "        # then update y, and minimize with respect to z\n",
    "    \n",
    "        for i in range(m):\n",
    "            a = A[i,:,:].flatten()\n",
    "            b = np.outer(y,z).flatten()\n",
    "            x[i] = np.dot(a,b)/np.dot(b,b)\n",
    "        \n",
    "        for j in range(n):\n",
    "            a = A[:,j,:].flatten()\n",
    "            b = np.outer(x,z).flatten()\n",
    "            y[j] = np.dot(a,b)/np.dot(b,b)\n",
    "            \n",
    "        for l in range(k):\n",
    "            a = A[:,:,l].flatten()\n",
    "            b = np.outer(x,y).flatten()\n",
    "            z[l] = np.dot(a,b)/np.dot(b,b)\n",
    "    \n",
    "    return x,y,z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: create a random tensor, test the method, and illustrate convergence and error\n",
    "\n",
    "a=np.array([1,2])\n",
    "b=np.array([3,4])\n",
    "c=np.outer(a,b)\n",
    "d=np.array([5,6,7])\n",
    "slices=[]\n",
    "for i in range(len(d)):\n",
    "    slices.append(c*d[i])\n",
    "\n",
    "cd=slices[0]\n",
    "for i in range(1,len(d)):\n",
    "    cd=np.array([cd,slices[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.60537108 3.99709405 3.17387092 2.80858305]\n",
      "[0.27709338 0.25628853 0.2641517 ]\n",
      "[0.59698531 0.58897538 0.56274477 0.63053578 0.48036481]\n"
     ]
    }
   ],
   "source": [
    "# run it on a randomly generated small tensor\n",
    "Atest=np.random.rand(60).reshape(4,3,5)\n",
    "a,b,c=CPdecomp(Atest,500)\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.72193108 0.66772671 0.68821321]\n",
      " [1.10756831 1.02440935 1.0558392 ]\n",
      " [0.87945863 0.8134267  0.83838341]\n",
      " [0.77823978 0.71980761 0.741892  ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.47637592057550737"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# explaining power of the rank-1 decomposition: not so good.\n",
    "# In practice we can try iterating the rank-1 approximation on the residual at each step\n",
    "# which is what we will do in the following\n",
    "\n",
    "ab=np.outer(a,b)\n",
    "reconstructed=np.outer(ab,c).reshape(4,3,5)\n",
    "np.linalg.norm(Atest-reconstructed)/np.linalg.norm(Atest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data from 12 stations and form 3rd-order tensor\n",
    "\n",
    "stationnames = ['Aotizhongxin', 'Changping', 'Dingling', 'Dongsi', 'Guanyuan',\\\n",
    "                'Gucheng', 'Huairou', 'Nongzhanguan', 'Shunyi', 'Tiantan',\\\n",
    "               'Wanliu', 'Wanshouxigong']\n",
    "datacols = ['PM2.5', 'PM10', 'SO2', 'NO2', 'CO', 'O3', 'TEMP', 'PRES', 'DEWP', 'WSPM']\n",
    "\n",
    "A=[]\n",
    "for station in stationnames:\n",
    "    data=pd.read_csv(\"/Users/yujiazhang/Desktop/CS 6241/Homework 2/AirData/\"+station+\".csv\")\n",
    "    B=data[datacols].values\n",
    "    A.append(B)\n",
    "\n",
    "C=np.array([A[0],A[1],A[2],A[3],A[4],A[5],A[6],A[7],A[8],A[9],A[10],A[11]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35064, 18)"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What the dataframe actually looks like:\n",
    "data.head(10)\n",
    "data.shape\n",
    "\n",
    "# We take the numerical values from\n",
    "# columns ['PM2.5', 'PM10', 'SO2', 'NO2', 'CO', 'O3', 'TEMP', 'PRES', 'DEWP', 'WSPM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 35064, 10)"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of data array before cleaning: (12, 35064, 10)\n",
      "number of invalid data rows dropped: 17946\n",
      "shape of data array after dropping NaN values: (12, 17118, 10)\n"
     ]
    }
   ],
   "source": [
    "# Clean up the data: brutally drop all slices with NaN values\n",
    "# This problem might be attacked with matrix completion methods in the future?\n",
    "\n",
    "D=C[:,range(35064),:]\n",
    "print('shape of data array before cleaning: '+str(D.shape))\n",
    "droplist = []\n",
    "for j in range(D.shape[1]):\n",
    "    if sum(np.isnan(D[:,j,:]).any(axis=1)) != 0:\n",
    "        droplist.append(j)\n",
    "\n",
    "print('number of invalid data rows dropped: '+str(len(droplist)))\n",
    "valid = np.delete(range(35064), droplist)\n",
    "\n",
    "D=D[:,valid,:]\n",
    "print('shape of data array after dropping NaN values: '+str(D.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process the data: normalize to center around mean\n",
    "# because each of the 10 metrics are of different scales, we normalize each [:,:,l] slice\n",
    "\n",
    "m,n=D[:,:,0].shape\n",
    "\n",
    "for l in range(D.shape[2]):\n",
    "    D[:,:,l] = np.matmul( (np.identity(m) - np.ones((m, m)) / m), D[:,:,l])\n",
    "    \n",
    "    # normalizing the variance causes NaN values. I haven't figured out why,\n",
    "    # but for now let's just not do it.\n",
    "    \n",
    "    #for j in range(D.shape[1]):\n",
    "        #D[:,j,l] = D[:,j,l] / np.linalg.norm(D[:,j,l])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute first-order approximation\n",
    "\n",
    "xD,yD,zD = CPdecomp(D,100)\n",
    "\n",
    "print(xD)\n",
    "print(yD)\n",
    "print(zD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8045828650784138"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first-order error rate is huge\n",
    "r1approx=np.outer(np.outer(xD,yD),zD).reshape(m,n,10)\n",
    "np.linalg.norm(D-r1approx)/np.linalg.norm(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7060123956552663"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute second-order approximation. Error rate decreases by ~10%\n",
    "xD2, yD2, zD2 = CPdecomp(D-r1approx, 100)\n",
    "r2approx = np.outer(np.outer(xD2, yD2), zD2).reshape(m,n,10)\n",
    "np.linalg.norm(D-r1approx-r2approx)/np.linalg.norm(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6283587427024288"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute third-order approximation. Error rate decreases by ~8%\n",
    "xD3, yD3, zD3 = CPdecomp(D-r1approx-r2approx, 100)\n",
    "r3approx = np.outer(np.outer(xD3, yD3), zD3).reshape(m,n,10)\n",
    "np.linalg.norm(D-r1approx-r2approx-r3approx)/np.linalg.norm(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5596077240231145"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute fourth-order approximation. Error rate decreases by ~7%\n",
    "xD4, yD4, zD4 = CPdecomp(D-r1approx-r2approx-r3approx, 100)\n",
    "r4approx = np.outer(np.outer(xD4, yD4), zD4).reshape(m,n,10)\n",
    "np.linalg.norm(D-r1approx-r2approx-r3approx-r4approx)/np.linalg.norm(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49863287138163437"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute fifth-order approximation. Error rate decreases by ~6%\n",
    "xD5, yD5, zD5 = CPdecomp(D-r1approx-r2approx-r3approx-r4approx, 100)\n",
    "r5approx = np.outer(np.outer(xD5, yD5), zD5).reshape(m,n,10)\n",
    "np.linalg.norm(D-r1approx-r2approx-r3approx-r4approx-r5approx)/np.linalg.norm(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43870000258546044"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute sixth-order approximation. Error rate decreases by ~6%\n",
    "xD6, yD6, zD6 = CPdecomp(D-r1approx-r2approx-r3approx-r4approx-r5approx, 100)\n",
    "r6approx = np.outer(np.outer(xD6, yD6), zD6).reshape(m,n,10)\n",
    "np.linalg.norm(D-r1approx-r2approx-r3approx-r4approx-r5approx-r6approx)/np.linalg.norm(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3781907284173674"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute seventh-order approximation. Error rate decreases by ~6%\n",
    "xD7, yD7, zD7 = CPdecomp(D-r1approx-r2approx-r3approx-r4approx-r5approx-r6approx, 100)\n",
    "r7approx = np.outer(np.outer(xD7, yD7), zD7).reshape(m,n,10)\n",
    "np.linalg.norm(D-r1approx-r2approx-r3approx-r4approx-r5approx-r6approx-r7approx)/np.linalg.norm(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3179015670765071"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute eighth-order approximation. Error rate decreases by ~6%\n",
    "xD8, yD8, zD8 = CPdecomp(D-r1approx-r2approx-r3approx-r4approx-r5approx-r6approx-r7approx, 100)\n",
    "r8approx = np.outer(np.outer(xD8, yD8), zD8).reshape(m,n,10)\n",
    "np.linalg.norm(D-r1approx-r2approx-r3approx-r4approx-r5approx-r6approx-r7approx-r8approx)/np.linalg.norm(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2479413102266508"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute ninth-order approximation. Error rate decreases by ~7%\n",
    "xD9, yD9, zD9 = CPdecomp(D-r1approx-r2approx-r3approx-r4approx-r5approx-r6approx-r7approx-r8approx, 100)\n",
    "r9approx = np.outer(np.outer(xD9, yD9), zD9).reshape(m,n,10)\n",
    "np.linalg.norm(D-r1approx-r2approx-r3approx-r4approx-r5approx-r6approx-r7approx-r8approx-r9approx)/np.linalg.norm(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18252055157010888"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute tenth-order approximation. Error rate decreases by ~6%\n",
    "xD10, yD10, zD10 = CPdecomp(D-r1approx-r2approx-r3approx-r4approx-r5approx-r6approx-r7approx-r8approx-r9approx, 100)\n",
    "r10approx = np.outer(np.outer(xD10, yD10), zD10).reshape(m,n,10)\n",
    "np.linalg.norm(D-r1approx-r2approx-r3approx-r4approx-r5approx-r6approx-r7approx-r8approx-r9approx-r10approx)/np.linalg.norm(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Results and interpretation\n",
    "\n",
    "Now let us try to intepret the factors qualitatively. Recall that $x\\in\\mathbb{R}^{12}$ corresponds to the twelve stations at which measurements were taken, and $z\\in\\mathbb{R}^{10}$ corresponds to the ten metrics for air quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 30.52604223  -0.82265533   1.30872902 -11.85116029 -16.60384968]\n",
      " [-49.49299978  44.67120413  -0.238763   -11.06995116  19.33304633]\n",
      " [-83.56880777  11.8830623   -8.73379534  -0.52723091 -13.03601823]\n",
      " [ 39.27508733 -12.04687417  -5.94851484   0.15686066   4.21206407]\n",
      " [ 24.19615187   3.92645797  -2.80933181   2.22256751  -5.20826871]\n",
      " [  4.87205552  24.03180142   8.5397304   14.31708718  -2.98689033]\n",
      " [-75.24318155 -24.94525532  -0.25830754   8.47918408  -2.32512246]\n",
      " [ 44.87436845 -13.02470072  -3.50256444  -7.97327105  -1.43734839]\n",
      " [-30.76052355 -55.92034969  10.83761846  -6.19459726   6.7394871 ]\n",
      " [ 31.06848019  -6.71721856  -5.82691661   5.41534848   7.67244142]\n",
      " [ 25.95235996  34.12667502  10.15571459  -0.66726431  -5.3198347 ]\n",
      " [ 38.30096709  -5.16214704  -3.5235989    7.69242707   8.96029357]]\n"
     ]
    }
   ],
   "source": [
    "# Let's look at the top 5 x-factors and try to interpret them\n",
    "# Recall: x is 12-dimensional, corresponding to the 12 stations\n",
    "\n",
    "print(np.c_[xD.reshape(len(xD),1), xD2.reshape(len(xD),1), xD3.reshape(len(xD),1), xD4.reshape(len(xD),1),\\\n",
    "           xD5.reshape(len(xD),1)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.13721292e-01  1.10139465e-01  3.55230376e-01  1.96462026e-01]\n",
      " [ 2.61068127e-01  1.29004968e-01  4.68613905e-01  2.44178836e-01]\n",
      " [ 4.08322450e-02  2.15398199e-02  6.39513243e-02  3.55188581e-02]\n",
      " [ 1.45770557e-01  4.15394352e-02  2.09181012e-01  1.00650894e-01]\n",
      " [ 4.59606642e+00  2.98846310e+00  1.26829437e+01  6.68225919e+00]\n",
      " [-3.95095161e-02 -1.73207678e-02 -8.73495635e-02 -2.42272170e-02]\n",
      " [ 1.91165806e-04 -2.94027005e-04 -4.69651753e-03 -1.07998298e-03]\n",
      " [ 1.10048904e-02 -7.40783211e-04  8.15779987e-04 -2.30845342e-04]\n",
      " [ 5.67604042e-03  1.92419494e-03  7.21862620e-03  1.97160789e-03]\n",
      " [-6.99720119e-04 -3.02006360e-04 -2.11901158e-03 -7.04764190e-04]]\n"
     ]
    }
   ],
   "source": [
    "# And the top 4 z-factors\n",
    "# Recall: z is 10-dimensional, corresponding to the 10 air quality metrics\n",
    "\n",
    "print(np.c_[zD.reshape(len(zD),1), zD2.reshape(len(zD),1), zD3.reshape(len(zD),1), zD4.reshape(len(zD),1)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Future work\n",
    "\n",
    "1. Use matrix completion to deal with missing data.\n",
    "2. Take into account the relative geographical locations of the 12 stations. Maybe look at pairwise distance between the stations, and also account for altitude, population density, traffic flow density, etc.\n",
    "3. Normalize the data with respect to its standard deviation. \n",
    "4. Constrain to nonnegative factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
